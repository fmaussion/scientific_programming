{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we are going to learn how to check that the code you write is working as expected. This will be achieved with the introduction of testing tools which will help you to formalize and organize your tests, as well as with the definition of new concepts: unit testing and test driven development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nobody can write bug-free code on the first try. Believe me on that one, **nobody** can. Therefore, every programmer constantly needs to test that newly written code is working as expected. Very often, scientists without formal training in computer sciences and programming will test their code \"ad-hoc\", i.e. with trial-and-error approaches on an interactive console or with print statements throughout the code. While this is a quick way to get the job done at first sight, it is in fact a dangerous, non-scientific practice. Today and throughout the lecture I will often remind you of a simple rule:\n",
    "\n",
    "**Always write tests for your code.**\n",
    "\n",
    "Oh, sorry, I actually meant to say this louder:\n",
    "\n",
    "```{admonition} Important\n",
    ":class: warning\n",
    "\n",
    "**Always write tests for your code.** Really.\n",
    "\n",
    "```\n",
    "\n",
    "Now, let's learn:\n",
    "- why should we write tests?\n",
    "- how should we write tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why testing? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scientific code is used to take important decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scientists are writing highly influencial code. Our climate and weather models are guiding political decisions, and, more recently, epidemiological models were [at the core of the decision making process that drove us into lockdown](https://www.nature.com/articles/d41586-020-01003-6). \n",
    "\n",
    "As scientists, we always have to provide results which are correct *to the best of our knowledge*. In recent years, erroneous software code led to several \"scandals\" in the scientific world. For example, in 2006 scientists [had to retract five of their papers](http://science.sciencemag.org/content/314/5807/1856) after discovering that their results were based on erroneous computations. \"As a result of a bug in a Perl script\", other authors [retracted a paper in PLoS](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0030158). In [this article](https://www.vice.com/en_us/article/zmjwda/a-code-glitch-may-have-caused-errors-in-more-than-100-published-studies) from October 2019, Vice reports how a short script that produces different results on different operating systems may have caused widespread errors in peer-reviewed studies. \n",
    "\n",
    "All these examples tell us an important message: the challenge of [(ir)reproducible research](https://www.nature.com/collections/prbfkwmwvz/) can only be tackled with **openly tested scientific software**.\n",
    "\n",
    "The requirement of \"100% error-free code\" announced in my title will of course never be achieved, but we have to do our best to avoid situations like described above. This slide by [Pietro Berkes](https://pberkes.github.io) presented at a SciPy tutorial brings it quite well to the point:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/10_errors_in_science.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Trial and error\" development practices hide important bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that \"the curve looks okay\" or that \"the relative humidity values seem realistic\" is not a quantitative test: it is error prone and possibly wrong. Testing must be achieved in a formal and quantitative way: \n",
    "- are the results of my code providing correct results as provided by an external source or an independent computation?\n",
    "- is my software running under all circumstances, also with missing data?\n",
    "- does my software fail with informative error messages when the input is not compliant to predefined rules?\n",
    "- is my software doing what is announced in the documentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code that worked once might not work a year later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a problem often overseen by scientists: once written and tested \"ad-hoc\", a script will be used and sometimes shared with colleagues. However, the same code might provide different results at a later time, as shown in these examples:\n",
    "- a change of computer leads to numerical accuracy errors which didn't happen before\n",
    "- numpy was updated to a new major version, and a function that worked perfectly now raises a ``DeprecationWarning`` when executed\n",
    "- the weather station was updated and suddenly the relative humidity values computed by my code are unrealistic. My code did not warn me about it and I used wrong values to compute the latent energy flux\n",
    "- I optimized my function ``clever_compute()`` which was too slow, and now it doesn't work anymore\n",
    "- I changed the function ``abc()`` where I found a bug, but now the other function ``dfg()`` does not work anymore \n",
    "\n",
    "In all these cases, formal tests would have helped to isolate the problems at an early stage and avoid late and tedious debugging. Therefore, tests are sometimes called \"regression tests\": once a test is written and works, it serves the purpose to make sure that future changes won't break things. Tests should be re-run each time something is changed in the code or when third-party packages are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untested <s>science</s> code is \"bad <s>science</s> code\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would never use data from an uncalibrated temperature sensor, would you? **Scientific code is no different: never trust untested code**. Don't trust tested code either (!), but untested code is much worse. When using new scripts/packages for your work, always check that these come with good tests for the functionality you are using. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing tests: a first example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions and exceptions \n",
    "<a id='asserts'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to test your code is to write **assertions** *within* your scripts and functions. Let's say we would like to write a function converting degrees Fahrenheit to degrees Celsius:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2c(tf):\n",
    "    \"\"\"Converts degrees Fahrenheit to degrees Celsius\"\"\"\n",
    "    r = (tf - 32) * 5/9\n",
    "    # Check that we provide physically reasonable results\n",
    "    assert r > -273.15\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assert statements are a convenient way to insert debugging assertions into a program. During the process of writing code, they help to check that some variables comply to certain rules you decided are important. Here are some examples of assert statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 2.\n",
    "assert result > 0\n",
    "assert type(result) == float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asserts are convenient, but they shouldn't be used too often. Indeed, my example above is a good example of bad code, since it might fail because of bad user input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2c(-1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very informative. A much better practice would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2c(tf):\n",
    "    \"\"\"Converts degrees Fahrenheit to degrees Celsius\"\"\"\n",
    "    if tf < -459.67:\n",
    "        raise ValueError('Input temperature below absolute zero!')\n",
    "    r = (tf - 32) * 5/9\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions in code are similar to debug ``print()`` statements filling a script: they help to debug or understand an algorithm, but they are not sustainable and should not replace real, independent tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first independent test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how should we test our ``f2c`` function appropriatly? Well, this should happen in a dedicated script or module testing that the function behaves like expected. A good start is to look for [typical conversion table values](https://en.wikipedia.org/wiki/Conversion_of_units_of_temperature#Comparison_of_temperature_scales) and see if our function returns the expected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert f2c(9941) == 5505\n",
    "assert f2c(212) == 100\n",
    "assert f2c(32) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the assertions fail it would let us know that something is wrong in our function. These tests, however, are still considered \"ad-hoc\": if you type them in the interpreter at the moment you write the function (\"trial and error development\") but don't store them for later, you might brake your code later on without noticing. This is why **test automation** is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pytest](https://docs.pytest.org/en/latest/contents.html) is a python package helping you to formalize and run your tests efficiently. It is a command line program recognizing and executing tests on demand as well as a python library providing useful modules helping programmers to write better tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{exercise}\n",
    "\n",
    "**Install the pytest package** (i.e. ``conda install pytest``). In your working directory (*see the warning about directories below*), create a new ``metutils.py`` module, which provides the ``f2c`` function. In the same directory, create a new ``test_metutils.py`` module, which contains a ``test_f2c`` function. This function must implement the tests we described above (*note: don't forget to import metutils in the test module!*). Now, **in a command line environment** (not in python!), type: \n",
    "\n",
    "```\n",
    "$ pytest\n",
    "```\n",
    "\n",
    "\n",
    "You should see an output similar to:\n",
    "\n",
    "```\n",
    "============================= test session starts ==============================\n",
    "platform linux -- Python 3.6.4, pytest-3.4.2, py-1.5.2, pluggy-0.6.0\n",
    "rootdir: /home/c7071047/ptest, inifile:\n",
    "collected 1 item                                                               \n",
    "\n",
    "test_metutils.py .                                                       [100%]\n",
    "\n",
    "=========================== 1 passed in 0.02 seconds ===========================\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did ``pytest`` just do? It scanned the working directory searching for certain patterns (files and functions starting with ``test_*``) and executed them. \n",
    "\n",
    "```{Warning} \n",
    "\n",
    "Pytest does a **recursive search** of all files and folders in the directory you start the script from. **Do not start pytest from your home directory or any directory full of non relevant files!** Otherwise it might take a lot of time and reach folders where it should not go, etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    "Now write a new function in the ``metutils`` module called ``c2f``, converting Celsius to Fahrenheit. Write a new test for this function. Then, write a third test function (``test_roundtrip_f2c``), which tests that for any valid value the \"roundtrip\" ``val == f2c(c2f(val))`` is true. Run ``pytest`` to see that everything is working as expected.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions in pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have read carefully you probably noticed that in [this section](#asserts) I recommended *against* heavy use of ``assert`` statements in your code. So what about all these assertions in our tests? Well, ``assert`` statements in tests executed by ``pytest`` are very different: they provide enriched information provided by the pytest package itself.\n",
    "\n",
    "```{exercise}\n",
    "Make a test fail, either by altering the original function, or by writing a wrong assert statement. Verify that the ``pytest`` log is giving you information about *why* the test is now failing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running pytest from ipython "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not necessary to open a terminal to run pytest. You can do so from the ipython interpreter with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\n",
      "Matplotlib: 3.4.3\n",
      "Freetype: 2.6.1\n",
      "rootdir: /home/mowglie/disk/Dropbox/HomeDocs/git/scientific_programming/book/week_06\n",
      "plugins: anyio-3.3.4, mpl-0.130\n",
      "collected 0 items                                                              \u001b[0m\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.19s\u001b[0m\u001b[33m =============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software testing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be surprised to learn that [there is a broad literature](https://www.google.at/search?q=software+testing+books) dedicated to software testing only. Authors and researchers agreed upon certain semantics and standards, and we are going to introduce a few of these concepts here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Unit tests](https://en.wikipedia.org/wiki/Unit_testing) have the purpose to test the smallest possible units of source code, i.e. single *functions* in python. Unit tests have the advantage to be highly specialized and are often written together or before the actual function from which they are testing the functionality.\n",
    "\n",
    "Unit tests are useful because they encourage programmers to write small \"code units\" (functions) instead of monolithic code. Let's make an example: say you'd like to write a function which computes the [dewpoint temperature](https://en.wikipedia.org/wiki/Dew_point#Calculating_the_dew_point) from temperatures (in Fahrenheit) and relative humidity (in %). This function will first convert °F to °C (a first code unit), then the dewpoint temperature in °C (another unit), and convert back to °F (a third unit). A good way to write this tool would be to write the three smallest functions first and test them independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Integration tests](https://en.wikipedia.org/wiki/Integration_testing) check that the units are working as a group. To keep the dewpoint example from above, an integration test would be a test that checks that the entire computation chain works as expected.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/yHGn1.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Verification](https://en.wikipedia.org/wiki/Software_verification_and_validation) is the process of checking that a software system meets the **specifications**. Software specification is an important concept in engineering and commercial applications, where software is built to meet the customer's needs. \n",
    "\n",
    "In science, the verification tests would check that a function really meets the documented features. For example: does my function really work with arrays of any dimension (as announced in the documentation) or only with scalars? Does my function really fail for relative humidity values below 0%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you spend some time on the wikipedia articles linked above you will see that there is a large variety of concepts behind software testing practices. For us scientists these concepts are only partly applicable because often we do not have to meet customer requirements the same way software developers do. For now we will stick with these three concepts and try to apply them as thoroughly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing and writing tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning you will probably find writing tests hard and confusing. \"What should I test and how?\" are the questions you will have to ask yourself very often.\n",
    "\n",
    "In the course of the semester this task will become easier and easier thanks to an influential testing philosophy: [test-driven development](https://en.wikipedia.org/wiki/Test-driven_development) (TDD). In TDD, one writes the test *before* coding the actual feature one would like to implement. Together with the function's documentation, this encourages to thoroughly think about the **design of a function's interface** before writing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/10_tdd.png\" width=\"40%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small> Chart by [Pietro Berkes](https://docplayer.net/10394130-Writing-robust-scientific-code-with-testing-and-python-pietro-berkes-enthought-uk.html) </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"AAA\"  concept "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle all unit tests follow a very similar structure, the Arrange-Act-Assert workflow:\n",
    "- **Arrange**: preparation of the data and objects needed for the test. For example, for the ``f2c`` test we needed to gather the data in °F (to be converted) and °C (to be compared to). This step is the most complicated and can sometimes occupy most of the test function\n",
    "- **Act**: execute the test. Basically a call to the targeted function using the input data we created in the previous step. Usually one line of code.\n",
    "- **Assert**: last step of a unit test application. Check and verify that the returned result is equal (or close to) the expected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for equality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most obvious kind of test and the one we used above. Following the AAA structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "tf = 212\n",
    "expected = 100\n",
    "# Act\n",
    "result = f2c(212)\n",
    "# Assert\n",
    "assert result == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most scientific applications, *exact* solutions cannot be guaranteed (one reason is floating point accuracy as we will learn later in the course). Therefore, ``numpy`` comes with several handy [testing functions](https://numpy.org/doc/stable/reference/routines.testing.html). We first need to make our function a little more flexible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Make f2c numpy friendly\n",
    "def f2c(tf):\n",
    "    \"\"\"Converts degrees Fahrenheit to degrees Celsius.\n",
    "\n",
    "    Works with numpy arrays as well as with scalars.\n",
    "    \"\"\"\n",
    "    if np.any(tf < -459.67):\n",
    "        raise ValueError('Input temperature below absolute zero!')\n",
    "    r = (tf - 32) * 5/9\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "tf = np.array([32, 212, 9941])\n",
    "expected = np.array([0, 100, 5505])\n",
    "# Act\n",
    "result = f2c(tf)\n",
    "# Assert\n",
    "np.testing.assert_allclose(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other very useful assert functions are listed in the [numpy documentation](https://numpy.org/doc/stable/reference/routines.testing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the validity of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "tf = np.array([32, 212, 9941])\n",
    "# Act\n",
    "result = f2c(tf)\n",
    "# Assert\n",
    "assert np.all(~np.isnan(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the interface of a function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These kind of tests verify that a function complies with the documentation. The focus is not necessarily the result, but also the type of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "tf = np.array([32, 212, 9941])\n",
    "# Act\n",
    "result = f2c(tf)\n",
    "# Assert\n",
    "assert type(tf) == type(result)\n",
    "assert len(tf) == len(result)\n",
    "\n",
    "# Arrange\n",
    "tf = 32.\n",
    "# Act\n",
    "result = f2c(tf)\n",
    "# Assert\n",
    "assert type(tf) == type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing that a function raises exceptions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tests are also testing the behavior of a function rather than its input. They are extremely important for libraries like numpy or matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "with pytest.raises(ValueError):\n",
    "    f2c(-3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, even more precise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError, match=\"Input temperature below absolute zero\"):\n",
    "    f2c(-3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing tests is hard. Programmers often spend as much time writing tests as writing actual code. Scientists cannot spend that much time writing tests of course, but I strongly believe that learning to write tests is a very good investment in the long term.\n",
    "\n",
    "I often hear the question: \"In science we are programming new models and equations, we can't test for that\". This is only partly true, and should not refrain you from writing unit tests. In particular, automated tests that check that data types are conserved, that a function returns valid values, or that a function simply *runs* are already much better than no test at all.\n",
    "\n",
    "Some parts of the code are really hard to test. For example:\n",
    "- data Input/Output (how do I know that I read the data correctly?)\n",
    "- model applied to real-case situations with unknown theoretical outcome\n",
    "- graphical output\n",
    "\n",
    "For these cases (and other issues such as automated testing and continuous integration) we might get back to the topic in a second testing chapter at the end of the semester (if time permits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take home points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **write tests!**\n",
    "- tests are a safeguard against scientific mistakes and future bugs\n",
    "- tests functions are written in a dedicated module and follow a simple convention: ``test_*``. They are run automatically with ``pytest``\n",
    "- \"unit testing\" is the practice of testing the smallest units of the code first. Unit tests help to organize the code.\n",
    "- tests can check for function output, behavior, and errors\n",
    "- testing is hard\n",
    "- **write tests!** Really. Write tests as you code, and you will be a better programmer and scientist."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": "18",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
